{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from typing import List, Dict, Any\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Diretórios principais\n",
    "BASE_DIR = os.getcwd()\n",
    "DIR_PAI = os.path.dirname(BASE_DIR)\n",
    "DIR_SRC = os.path.join(DIR_PAI, 'src')\n",
    "DIR_VECTORSTORE = os.path.join(DIR_SRC, 'vector_store') \n",
    "DIR_DATA = os.path.join(DIR_PAI, \"data\")\n",
    "DIR_DATA_RAW = os.path.join(DIR_DATA, \"raw\")\n",
    "DIR_LOGS = os.path.join(DIR_DATA, \"logs\")\n",
    "DIR_DATA_REFINEMENT = os.path.join(DIR_DATA, \"outputs_vision_and_extractor\")\n",
    "\n",
    "class CollectionCreator:\n",
    "    def __init__(self, config_file: str):\n",
    "        self.config = self.load_config(config_file)\n",
    "        load_dotenv()\n",
    "        self.qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "        self.qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "        if not self.qdrant_url or not self.qdrant_api_key:\n",
    "            raise ValueError(\"Variáveis QDRANT_URL ou QDRANT_API_KEY não encontradas!\")\n",
    "\n",
    "        self.file_to_metadata = self.load_metadata_from_csv()\n",
    "\n",
    "        # Conexão com o cliente Qdrant\n",
    "        self.qdrant_client = QdrantClient(\n",
    "            url=self.qdrant_url,\n",
    "            api_key=self.qdrant_api_key\n",
    "        )\n",
    "\n",
    "    def load_config(self, config_file: str) -> Dict[str, Any]:\n",
    "        \"\"\"Carrega o arquivo de configuração JSON.\"\"\"\n",
    "        with open(config_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def load_metadata_from_csv(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Carrega o CSV e cria um mapeamento arquivo_id -> metadados.\"\"\"\n",
    "        csv_path = os.path.join(DIR_DATA, \"subcategoria_name.csv\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        metadata_dict = df.set_index(\"arquivo_id\").to_dict(orient=\"index\")\n",
    "        print(\"Metadados carregados do CSV:\", metadata_dict)\n",
    "        return metadata_dict\n",
    "\n",
    "    def load_json_documents(self, file_id: str) -> List[Document]:\n",
    "        \"\"\"Carrega arquivos JSON, adiciona metadados do CSV e páginas.\"\"\"\n",
    "        dir_path = os.path.join(self.config['pdf_dir'], file_id)\n",
    "\n",
    "        if not os.path.isdir(dir_path):\n",
    "            print(f\"Diretório {dir_path} não existe. Pulando.\")\n",
    "            return []\n",
    "\n",
    "        documents = []\n",
    "        for fname in os.listdir(dir_path):\n",
    "            if fname.endswith(\"_resultado.json\"):\n",
    "                fpath = os.path.join(dir_path, fname)\n",
    "                with open(fpath, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                text = data.get(\"unified_analysis\", \"\")\n",
    "                page_number = data.get(\"page\", None)\n",
    "                metadata = {\"source\": fname, \"pag\": page_number}\n",
    "\n",
    "                try:\n",
    "                    arquivo_id = int(fname.split(\"_\")[1])\n",
    "                    metadata[\"arquivo_id\"] = arquivo_id\n",
    "                    if arquivo_id in self.file_to_metadata:\n",
    "                        csv_metadata = self.file_to_metadata[arquivo_id]\n",
    "                        metadata.update(csv_metadata)\n",
    "                except (IndexError, ValueError):\n",
    "                    print(f\"Erro ao extrair arquivo_id de {fname}\")\n",
    "\n",
    "                # Mover arquivo_id para o nível superior do payload\n",
    "                doc_payload = metadata.copy()\n",
    "                if \"arquivo_id\" in metadata:\n",
    "                    doc_payload[\"arquivo_id\"] = metadata.pop(\"arquivo_id\")\n",
    "\n",
    "                documents.append(Document(page_content=text, metadata=doc_payload))\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def clean_metadata(self, metadata: dict) -> dict:\n",
    "        \"\"\"Garante que o metadata seja serializável e limpo.\"\"\"\n",
    "        return {k: v for k, v in metadata.items() if isinstance(v, (str, int, float, bool, list, dict))}\n",
    "\n",
    "    def create_collection_and_index(self, collection_config: Dict[str, Any]):\n",
    "        \"\"\"Cria a coleção e o índice no Qdrant antes da inserção de dados.\"\"\"\n",
    "        collection_name = collection_config['collection_name']\n",
    "        embeddings_model = collection_config['embeddings_model']\n",
    "        local_embeddings = HuggingFaceEmbeddings(model_name=embeddings_model, model_kwargs={'trust_remote_code': True})\n",
    "\n",
    "        # Criar coleção se não existir\n",
    "        self.qdrant_client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE),\n",
    "            on_disk_payload=True  # Armazena payloads no disco\n",
    "        )\n",
    "\n",
    "        # Criar índice para os metadados\n",
    "        for field_name, field_type in [(\"arquivo_id\", models.PayloadSchemaType.KEYWORD)]:\n",
    "            try:\n",
    "                self.qdrant_client.create_payload_index(\n",
    "                    collection_name=collection_name,\n",
    "                    field_name=field_name,\n",
    "                    field_schema=field_type\n",
    "                )\n",
    "                print(f\"Índice criado para o campo '{field_name}' na coleção '{collection_name}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criar índice '{field_name}': {e}\")\n",
    "\n",
    "        # Processar documentos\n",
    "        documents = collection_config['documents']\n",
    "        chunk_size = collection_config['chunk_size']\n",
    "        chunk_overlap = collection_config['chunk_overlap']\n",
    "\n",
    "        if chunk_size == \"Page\":\n",
    "            splits = documents\n",
    "        else:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "            splits = text_splitter.split_documents(documents)\n",
    "\n",
    "        for doc in splits:\n",
    "            doc.metadata = self.clean_metadata(doc.metadata)\n",
    "\n",
    "        QdrantVectorStore.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=local_embeddings,\n",
    "            url=self.qdrant_url,\n",
    "            api_key=self.qdrant_api_key,\n",
    "            collection_name=collection_name,\n",
    "            force_recreate=False\n",
    "        )\n",
    "\n",
    "        print(f\"Collection {collection_name} criada com sucesso e dados indexados.\")\n",
    "\n",
    "    def generate_collection_configs(self, documents_dict: Dict[str, List[Document]]) -> List[Dict[str, Any]]:\n",
    "        embeddings_models = self.config['embeddings_models']\n",
    "        chunk_sizes = self.config['chunk_sizes']\n",
    "        chunk_overlaps = self.config['chunk_overlaps']\n",
    "\n",
    "        all_docs = []\n",
    "        for docs in documents_dict.values():\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        combos = itertools.product(embeddings_models, chunk_sizes, chunk_overlaps)\n",
    "        configs = []\n",
    "        for emb_model, c_size, c_overlap in combos:\n",
    "            chunk_display = c_size if c_size != \"Page\" else \"by-page\"\n",
    "            collection_name = f\"{self.config['collection_name']}_chunk{chunk_display}_overlap{c_overlap}_{emb_model.split('/')[-1]}\"\n",
    "            extraction_config = {\n",
    "                \"collection_name\": collection_name,\n",
    "                \"chunk_size\": c_size,\n",
    "                \"chunk_overlap\": c_overlap,\n",
    "                \"embeddings_model\": emb_model,\n",
    "                \"documents\": all_docs,\n",
    "            }\n",
    "            configs.append(extraction_config)\n",
    "\n",
    "        return configs\n",
    "\n",
    "    def create_collections(self):\n",
    "        documents_dict = {}\n",
    "        for arquivo_id in self.config['arquivo_ids_to_process']:\n",
    "            if not arquivo_id.startswith(\"fluidos_\"):\n",
    "                continue\n",
    "            docs = self.load_json_documents(arquivo_id)\n",
    "            if docs:\n",
    "                documents_dict[arquivo_id] = docs\n",
    "\n",
    "        self.collection_configs = self.generate_collection_configs(documents_dict)\n",
    "\n",
    "        for collection_config in self.collection_configs:\n",
    "            self.create_collection_and_index(collection_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadados carregados do CSV: {11484: {'subcategoria_nome': 'fluidos'}, 11640: {'subcategoria_nome': 'fluidos'}, 13271: {'subcategoria_nome': 'fluidos'}, 13417: {'subcategoria_nome': 'fluidos'}, 13472: {'subcategoria_nome': 'fluidos'}, 13572: {'subcategoria_nome': 'fluidos'}, 13852: {'subcategoria_nome': 'fluidos'}, 4802: {'subcategoria_nome': 'fluidos'}}\n",
      "Índice criado para o campo 'arquivo_id' na coleção 'fluidos_chunkby-page_overlap100_multilingual-e5-large'.\n",
      "Collection fluidos_chunkby-page_overlap100_multilingual-e5-large criada com sucesso e dados indexados.\n",
      "Índice criado para o campo 'arquivo_id' na coleção 'fluidos_chunkby-page_overlap0_multilingual-e5-large'.\n",
      "Collection fluidos_chunkby-page_overlap0_multilingual-e5-large criada com sucesso e dados indexados.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_file = os.path.join(DIR_VECTORSTORE, \"config.json\")\n",
    "    processor = CollectionCreator(config_file)\n",
    "    processor.create_collections()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
